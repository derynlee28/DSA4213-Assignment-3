{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, set_seed\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# from utils import compute_metrics, run_experiment\n",
    "\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ffaa012f35473192de7a2627fa9985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2087995cedd34b8da76a643ad1fa3ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "irony/train-00000-of-00001.parquet:   0%|          | 0.00/183k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8240bb51dc54b5e81a6f038cc2b13cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "irony/test-00000-of-00001.parquet:   0%|          | 0.00/54.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb1d26f2f1e4ea2beda0f9d70bafa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "irony/validation-00000-of-00001.parquet:   0%|          | 0.00/61.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89547b262934ca88b364df1a545b611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2862 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c41a182d5ff4c4fb6979e55f179747c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510f8ce557c14dd79eb3c5dbe2de6f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2862\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 784\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 955\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load tweet eval dataset\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Non-ironic (0): 1417 (49.5%)\n",
      "  Ironic (1): 1445 (50.5%)\n"
     ]
    }
   ],
   "source": [
    "# Class distribution\n",
    "\n",
    "train_labels = dataset['train']['label']\n",
    "print(f\"  Non-ironic (0): {train_labels.count(0)} ({train_labels.count(0)/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  Ironic (1): {train_labels.count(1)} ({train_labels.count(1)/len(train_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(pred.predictions), dim=-1)[:, 1].numpy()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"binary\"),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"roc_auc\": roc_auc_score(labels, probs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "\n",
    "def run_experiment(model_name, strategy, learning_rate=2e-5, batch_size=16, epochs=3, lora_config=None):\n",
    "    print(f\"\\nTraining {model_name} | {strategy} | lr={learning_rate} | batch_size={batch_size} | epochs={epochs}\")\n",
    "\n",
    "    # Initialize the tokenizer for this model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize dataset and convert to pytorch\n",
    "    tokenized_data = dataset.map(\n",
    "        lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=128),\n",
    "        batched=True\n",
    "    )\n",
    "    tokenized_data = tokenized_data.rename_column('label', 'labels')\n",
    "    tokenized_data.set_format('torch')\n",
    "\n",
    "    # Load models\n",
    "    if strategy == \"full\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    elif strategy == \"lora\":\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        # Have to determine target modules for LoRA\n",
    "        if model_name.startswith(\"distilbert\"):\n",
    "            target_modules = [\"q_lin\", \"v_lin\"]\n",
    "        elif model_name.startswith(\"roberta\"):\n",
    "            target_modules = [\"query\", \"value\"]\n",
    "        else:\n",
    "            target_modules = None\n",
    "\n",
    "        # LoRA configurations (create default ones if theres no lora_config)\n",
    "        if lora_config is None:\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\",\n",
    "                target_modules=target_modules)\n",
    "        else:\n",
    "            # If target_modules not set in the provided config, fill it\n",
    "            if getattr(lora_config, \"target_modules\", None) is None:\n",
    "                lora_config.target_modules = target_modules\n",
    "\n",
    "        model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable_params:,}/{total_params:,}\")\n",
    "\n",
    "    # Move model to GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{model_name}_{strategy}_lr{learning_rate}_bs{batch_size}_ep{epochs}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        logging_steps=100,\n",
    "        warmup_steps=100,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=0)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data['train'],\n",
    "        eval_dataset=tokenized_data['validation'],\n",
    "        compute_metrics=compute_metrics)\n",
    "\n",
    "    # Train & evaluate + track training time\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    eval_results = trainer.evaluate(tokenized_data['test'])\n",
    "    preds = trainer.predict(tokenized_data['test'])\n",
    "    y_pred = np.argmax(preds.predictions, axis=-1)\n",
    "    y_true = preds.label_ids\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    test_texts = dataset['test']['text']\n",
    "    df_test = pd.DataFrame({\n",
    "        'text': test_texts,\n",
    "        'true_label': y_true,\n",
    "        'pred_label': y_pred\n",
    "    })\n",
    "    error_cases = df_test[df_test['true_label'] != df_test['pred_label']]\n",
    "\n",
    "    return {\n",
    "        \"mode\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"model\": model_name,\n",
    "        \"strategy\": strategy,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"train_time_s\": train_time,\n",
    "        \"accuracy\": eval_results.get('eval_accuracy'),\n",
    "        \"precision\": eval_results.get('eval_precision'),\n",
    "        \"recall\": eval_results.get('eval_recall'),\n",
    "        \"f1\": eval_results.get('eval_f1'),\n",
    "        \"f1_macro\": eval_results.get('eval_f1_macro'),\n",
    "        \"roc_auc\": eval_results.get('eval_roc_auc'),\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"error_cases\": error_cases.head(10)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training distilbert-base-uncased | full | lr=2e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c08c27439241299a3b6587c3b0877d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b7b2fd36ed4760be1cd8efa5701551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb76373cf7764c4eb1b1b8ebcd2181bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b72c5989af84d67a7fcce4f81b5e958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41698f42c834e88a6e031e514c8a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2862 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a656eb16a482e8f75049b06d51659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db78e36845e44c1a87adab477d80cec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d28b911ded45fd8968200dbeeee819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 66,955,010/66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 01:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678900</td>\n",
       "      <td>0.633159</td>\n",
       "      <td>0.609424</td>\n",
       "      <td>0.563166</td>\n",
       "      <td>0.811404</td>\n",
       "      <td>0.664870</td>\n",
       "      <td>0.598432</td>\n",
       "      <td>0.716499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.598660</td>\n",
       "      <td>0.683770</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.717105</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.683769</td>\n",
       "      <td>0.753186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.425100</td>\n",
       "      <td>0.626694</td>\n",
       "      <td>0.695288</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.747807</td>\n",
       "      <td>0.700925</td>\n",
       "      <td>0.695180</td>\n",
       "      <td>0.764010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training distilbert-base-uncased | lora | lr=2e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5020bb694be4f3796075e9e57c2321e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 739,586/67,694,596\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.694500</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.578010</td>\n",
       "      <td>0.549533</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.593340</td>\n",
       "      <td>0.577410</td>\n",
       "      <td>0.624776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675600</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.588482</td>\n",
       "      <td>0.560229</td>\n",
       "      <td>0.642544</td>\n",
       "      <td>0.598570</td>\n",
       "      <td>0.588222</td>\n",
       "      <td>0.627751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.670176</td>\n",
       "      <td>0.584293</td>\n",
       "      <td>0.561587</td>\n",
       "      <td>0.589912</td>\n",
       "      <td>0.575401</td>\n",
       "      <td>0.584111</td>\n",
       "      <td>0.629021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training roberta-base | full | lr=2e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3d7b17ba8f46d082445151317d8d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877ba30854fe44bd83aeee461ee45a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5af7ef310642e1bcf41b16f5616338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1083fa711b30403fb9cd5a139d38a40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fc2defd1ee4151bfc91793e98bdd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6533c7152d4a35bf8c7dc404ea0176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2862 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274e0f115a85412099dd7aeb5743e4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae01a5e36d7f45fcbbceb71c7f2b1878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460a927188544a5cb033c9dbb7690b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 04:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.587299</td>\n",
       "      <td>0.681675</td>\n",
       "      <td>0.637184</td>\n",
       "      <td>0.774123</td>\n",
       "      <td>0.699010</td>\n",
       "      <td>0.680616</td>\n",
       "      <td>0.769605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.602435</td>\n",
       "      <td>0.717277</td>\n",
       "      <td>0.645768</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.753199</td>\n",
       "      <td>0.711158</td>\n",
       "      <td>0.830635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.574406</td>\n",
       "      <td>0.742408</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.741783</td>\n",
       "      <td>0.841877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training roberta-base | lora | lr=2e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bf671e5f3b4f37a43a0d760866894b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 887,042/125,534,212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 02:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693600</td>\n",
       "      <td>0.692309</td>\n",
       "      <td>0.543455</td>\n",
       "      <td>0.512853</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.646677</td>\n",
       "      <td>0.500854</td>\n",
       "      <td>0.582070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.690357</td>\n",
       "      <td>0.568586</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>0.813596</td>\n",
       "      <td>0.642981</td>\n",
       "      <td>0.549004</td>\n",
       "      <td>0.604832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.689239</td>\n",
       "      <td>0.579058</td>\n",
       "      <td>0.543269</td>\n",
       "      <td>0.743421</td>\n",
       "      <td>0.627778</td>\n",
       "      <td>0.571720</td>\n",
       "      <td>0.609161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, run the baseline experiments for distilbert and roberta\n",
    "\n",
    "baseline_results = []\n",
    "baseline_models = [\"distilbert-base-uncased\", \"roberta-base\"]\n",
    "\n",
    "for model_name in baseline_models:\n",
    "    for strategy in [\"full\", \"lora\"]:\n",
    "        lora_cfg = None\n",
    "        if strategy == \"lora\":\n",
    "            lora_cfg = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "        result = run_experiment(model_name, strategy, learning_rate=2e-5, batch_size=16, epochs=3, lora_config=lora_cfg)\n",
    "        baseline_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_baseline_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"PeftModelForSequenceClassification(\\n  (base_model): LoraModel(\\n    (model): DistilBertForSequenceClassification(\\n      (distilbert): DistilBertModel(\\n        (embeddings): Embeddings(\\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\\n          (position_embeddings): Embedding(512, 768)\\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n          (dropout): Dropout(p=0.1, inplace=False)\\n        )\\n        (transformer): Transformer(\\n          (layer): ModuleList(\\n            (0-5): 6 x TransformerBlock(\\n              (attention): DistilBertSdpaAttention(\\n                (dropout): Dropout(p=0.1, inplace=False)\\n                (q_lin): lora.Linear(\\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\\n                  (lora_dropout): ModuleDict(\\n                    (default): Dropout(p=0.1, inplace=False)\\n                  )\\n                  (lora_A): ModuleDict(\\n                    (default): Linear(in_features=768, out_features=8, bias=False)\\n                  )\\n                  (lora_B): ModuleDict(\\n                    (default): Linear(in_features=8, out_features=768, bias=False)\\n                  )\\n                  (lora_embedding_A): ParameterDict()\\n                  (lora_embedding_B): ParameterDict()\\n                  (lora_magnitude_vector): ModuleDict()\\n                )\\n                (k_lin): Linear(in_features=768, out_features=768, bias=True)\\n                (v_lin): lora.Linear(\\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\\n                  (lora_dropout): ModuleDict(\\n                    (default): Dropout(p=0.1, inplace=False)\\n                  )\\n                  (lora_A): ModuleDict(\\n                    (default): Linear(in_features=768, out_features=8, bias=False)\\n                  )\\n                  (lora_B): ModuleDict(\\n                    (default): Linear(in_features=8, out_features=768, bias=False)\\n                  )\\n                  (lora_embedding_A): ParameterDict()\\n                  (lora_embedding_B): ParameterDict()\\n                  (lora_magnitude_vector): ModuleDict()\\n                )\\n                (out_lin): Linear(in_features=768, out_features=768, bias=True)\\n              )\\n              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (ffn): FFN(\\n                (dropout): Dropout(p=0.1, inplace=False)\\n                (lin1): Linear(in_features=768, out_features=3072, bias=True)\\n                (lin2): Linear(in_features=3072, out_features=768, bias=True)\\n                (activation): GELUActivation()\\n              )\\n              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            )\\n          )\\n        )\\n      )\\n      (pre_classifier): ModulesToSaveWrapper(\\n        (original_module): Linear(in_features=768, out_features=768, bias=True)\\n        (modules_to_save): ModuleDict(\\n          (default): Linear(in_features=768, out_features=768, bias=True)\\n        )\\n      )\\n      (classifier): ModulesToSaveWrapper(\\n        (original_module): Linear(in_features=768, out_features=2, bias=True)\\n        (modules_to_save): ModuleDict(\\n          (default): Linear(in_features=768, out_features=2, bias=True)\\n        )\\n      )\\n      (dropout): Dropout(p=0.2, inplace=False)\\n    )\\n  )\\n)\",\n          \"PeftModelForSequenceClassification(\\n  (base_model): LoraModel(\\n    (model): RobertaForSequenceClassification(\\n      (roberta): RobertaModel(\\n        (embeddings): RobertaEmbeddings(\\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\\n          (token_type_embeddings): Embedding(1, 768)\\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n          (dropout): Dropout(p=0.1, inplace=False)\\n        )\\n        (encoder): RobertaEncoder(\\n          (layer): ModuleList(\\n            (0-11): 12 x RobertaLayer(\\n              (attention): RobertaAttention(\\n                (self): RobertaSdpaSelfAttention(\\n                  (query): lora.Linear(\\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\\n                    (lora_dropout): ModuleDict(\\n                      (default): Dropout(p=0.1, inplace=False)\\n                    )\\n                    (lora_A): ModuleDict(\\n                      (default): Linear(in_features=768, out_features=8, bias=False)\\n                    )\\n                    (lora_B): ModuleDict(\\n                      (default): Linear(in_features=8, out_features=768, bias=False)\\n                    )\\n                    (lora_embedding_A): ParameterDict()\\n                    (lora_embedding_B): ParameterDict()\\n                    (lora_magnitude_vector): ModuleDict()\\n                  )\\n                  (key): Linear(in_features=768, out_features=768, bias=True)\\n                  (value): lora.Linear(\\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\\n                    (lora_dropout): ModuleDict(\\n                      (default): Dropout(p=0.1, inplace=False)\\n                    )\\n                    (lora_A): ModuleDict(\\n                      (default): Linear(in_features=768, out_features=8, bias=False)\\n                    )\\n                    (lora_B): ModuleDict(\\n                      (default): Linear(in_features=8, out_features=768, bias=False)\\n                    )\\n                    (lora_embedding_A): ParameterDict()\\n                    (lora_embedding_B): ParameterDict()\\n                    (lora_magnitude_vector): ModuleDict()\\n                  )\\n                  (dropout): Dropout(p=0.1, inplace=False)\\n                )\\n                (output): RobertaSelfOutput(\\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n                  (dropout): Dropout(p=0.1, inplace=False)\\n                )\\n              )\\n              (intermediate): RobertaIntermediate(\\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\\n                (intermediate_act_fn): GELUActivation()\\n              )\\n              (output): RobertaOutput(\\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n                (dropout): Dropout(p=0.1, inplace=False)\\n              )\\n            )\\n          )\\n        )\\n      )\\n      (classifier): ModulesToSaveWrapper(\\n        (original_module): RobertaClassificationHead(\\n          (dense): Linear(in_features=768, out_features=768, bias=True)\\n          (dropout): Dropout(p=0.1, inplace=False)\\n          (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n        )\\n        (modules_to_save): ModuleDict(\\n          (default): RobertaClassificationHead(\\n            (dense): Linear(in_features=768, out_features=768, bias=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n            (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n          )\\n        )\\n      )\\n    )\\n  )\\n)\",\n          \"DistilBertForSequenceClassification(\\n  (distilbert): DistilBertModel(\\n    (embeddings): Embeddings(\\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\\n      (position_embeddings): Embedding(512, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (transformer): Transformer(\\n      (layer): ModuleList(\\n        (0-5): 6 x TransformerBlock(\\n          (attention): DistilBertSdpaAttention(\\n            (dropout): Dropout(p=0.1, inplace=False)\\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\\n          )\\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n          (ffn): FFN(\\n            (dropout): Dropout(p=0.1, inplace=False)\\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\\n            (activation): GELUActivation()\\n          )\\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n        )\\n      )\\n    )\\n  )\\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\\n  (dropout): Dropout(p=0.2, inplace=False)\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenizer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"[PAD]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t100: AddedToken(\\\"[UNK]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t101: AddedToken(\\\"[CLS]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t102: AddedToken(\\\"[SEP]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t103: AddedToken(\\\"[MASK]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"[PAD]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t100: AddedToken(\\\"[UNK]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t101: AddedToken(\\\"[CLS]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t102: AddedToken(\\\"[SEP]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n\\t103: AddedToken(\\\"[MASK]\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\\n}\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"roberta-base\",\n          \"distilbert-base-uncased\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"lora\",\n          \"full\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 2e-05,\n        \"max\": 2e-05,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 16,\n        \"max\": 16,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75.65035493067612,\n        \"min\": 74.29596781730652,\n        \"max\": 249.50429224967957,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          74.29596781730652\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08005202709832969,\n        \"min\": 0.5459183673469388,\n        \"max\": 0.7091836734693877,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5688775510204082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08313504355293447,\n        \"min\": 0.44630071599045346,\n        \"max\": 0.6169014084507042,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.4636118598382749\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08378144492802812,\n        \"min\": 0.5530546623794212,\n        \"max\": 0.729903536977492,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5530546623794212\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08202694146994262,\n        \"min\": 0.5043988269794721,\n        \"max\": 0.6576576576576577,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5043988269794721\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07951856871548241,\n        \"min\": 0.5437538823683263,\n        \"max\": 0.7024430195161903,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5614544924965081\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11052729941015105,\n        \"min\": 0.5751276316594496,\n        \"max\": 0.78453872456714,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5751276316594496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confusion_matrix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_baseline_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-acab2b5a-ab82-46d2-a354-910ac66dcb94\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>strategy</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DistilBertForSequenceClassification(\\n  (disti...</td>\n",
       "      <td>DistilBertTokenizerFast(name_or_path='distilbe...</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>109.737457</td>\n",
       "      <td>0.677296</td>\n",
       "      <td>0.573232</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.642150</td>\n",
       "      <td>0.674153</td>\n",
       "      <td>0.747429</td>\n",
       "      <td>[[304, 169], [84, 227]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PeftModelForSequenceClassification(\\n  (base_m...</td>\n",
       "      <td>DistilBertTokenizerFast(name_or_path='distilbe...</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>74.295968</td>\n",
       "      <td>0.568878</td>\n",
       "      <td>0.463612</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.504399</td>\n",
       "      <td>0.561454</td>\n",
       "      <td>0.575128</td>\n",
       "      <td>[[274, 199], [139, 172]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>249.504292</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.616901</td>\n",
       "      <td>0.704180</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>0.784539</td>\n",
       "      <td>[[337, 136], [92, 219]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PeftModelForSequenceClassification(\\n  (base_m...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>141.687181</td>\n",
       "      <td>0.545918</td>\n",
       "      <td>0.446301</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.512329</td>\n",
       "      <td>0.543754</td>\n",
       "      <td>0.577592</td>\n",
       "      <td>[[241, 232], [124, 187]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acab2b5a-ab82-46d2-a354-910ac66dcb94')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-acab2b5a-ab82-46d2-a354-910ac66dcb94 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-acab2b5a-ab82-46d2-a354-910ac66dcb94');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-d1b0f4cc-1e63-4e0a-9343-1d7b2ba37eec\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1b0f4cc-1e63-4e0a-9343-1d7b2ba37eec')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-d1b0f4cc-1e63-4e0a-9343-1d7b2ba37eec button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_79475f25-4883-4cae-bf21-b435ad77c020\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_baseline_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_79475f25-4883-4cae-bf21-b435ad77c020 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_baseline_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                mode  \\\n",
       "0  DistilBertForSequenceClassification(\\n  (disti...   \n",
       "1  PeftModelForSequenceClassification(\\n  (base_m...   \n",
       "2  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "3  PeftModelForSequenceClassification(\\n  (base_m...   \n",
       "\n",
       "                                           tokenizer                    model  \\\n",
       "0  DistilBertTokenizerFast(name_or_path='distilbe...  distilbert-base-uncased   \n",
       "1  DistilBertTokenizerFast(name_or_path='distilbe...  distilbert-base-uncased   \n",
       "2  RobertaTokenizerFast(name_or_path='roberta-bas...             roberta-base   \n",
       "3  RobertaTokenizerFast(name_or_path='roberta-bas...             roberta-base   \n",
       "\n",
       "  strategy  learning_rate  batch_size  epochs  train_time_s  accuracy  \\\n",
       "0     full        0.00002          16       3    109.737457  0.677296   \n",
       "1     lora        0.00002          16       3     74.295968  0.568878   \n",
       "2     full        0.00002          16       3    249.504292  0.709184   \n",
       "3     lora        0.00002          16       3    141.687181  0.545918   \n",
       "\n",
       "   precision    recall        f1  f1_macro   roc_auc          confusion_matrix  \n",
       "0   0.573232  0.729904  0.642150  0.674153  0.747429   [[304, 169], [84, 227]]  \n",
       "1   0.463612  0.553055  0.504399  0.561454  0.575128  [[274, 199], [139, 172]]  \n",
       "2   0.616901  0.704180  0.657658  0.702443  0.784539   [[337, 136], [92, 219]]  \n",
       "3   0.446301  0.601286  0.512329  0.543754  0.577592  [[241, 232], [124, 187]]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline_results = pd.DataFrame(baseline_results)\n",
    "df_baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     model strategy  accuracy  f1_macro  train_time_s\n",
      "2             roberta-base     full  0.709184  0.702443    297.390193\n",
      "0  distilbert-base-uncased     full  0.677296  0.674153    136.627033\n",
      "1  distilbert-base-uncased     lora  0.568878  0.561454     70.781740\n",
      "3             roberta-base     lora  0.545918  0.543754    136.509491\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine-tuning\n",
      "FULL: lr=2e-05, bs=16, epochs=3\n",
      "\n",
      "Training roberta-base | full | lr=2e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f882d50e59f346368f9454a312d4ff6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 06:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.587299</td>\n",
       "      <td>0.681675</td>\n",
       "      <td>0.637184</td>\n",
       "      <td>0.774123</td>\n",
       "      <td>0.699010</td>\n",
       "      <td>0.680616</td>\n",
       "      <td>0.769605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.602435</td>\n",
       "      <td>0.717277</td>\n",
       "      <td>0.645768</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.753199</td>\n",
       "      <td>0.711158</td>\n",
       "      <td>0.830635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.574406</td>\n",
       "      <td>0.742408</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.741783</td>\n",
       "      <td>0.841877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL: lr=3e-05, bs=16, epochs=3\n",
      "\n",
      "Training roberta-base | full | lr=3e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 05:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.589197</td>\n",
       "      <td>0.678534</td>\n",
       "      <td>0.632327</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.698724</td>\n",
       "      <td>0.677084</td>\n",
       "      <td>0.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>0.701571</td>\n",
       "      <td>0.632970</td>\n",
       "      <td>0.892544</td>\n",
       "      <td>0.740673</td>\n",
       "      <td>0.694628</td>\n",
       "      <td>0.835179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.581767</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.713208</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.766734</td>\n",
       "      <td>0.758908</td>\n",
       "      <td>0.850025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL: lr=4e-05, bs=16, epochs=3\n",
      "\n",
      "Training roberta-base | full | lr=4e-05 | batch_size=16 | epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 06:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677200</td>\n",
       "      <td>0.610679</td>\n",
       "      <td>0.671204</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.647191</td>\n",
       "      <td>0.669674</td>\n",
       "      <td>0.729011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.607442</td>\n",
       "      <td>0.713089</td>\n",
       "      <td>0.652685</td>\n",
       "      <td>0.853070</td>\n",
       "      <td>0.739544</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.571027</td>\n",
       "      <td>0.754974</td>\n",
       "      <td>0.713462</td>\n",
       "      <td>0.813596</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.754855</td>\n",
       "      <td>0.846395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run RobERTA\n",
    "\n",
    "roberta_results = []\n",
    "\n",
    "# Baseline\n",
    "baseline_lr = 2e-5\n",
    "baseline_full_bs = 16\n",
    "baseline_lora_bs = 16\n",
    "baseline_epochs = 3\n",
    "baseline_lora_dropout = 0.1\n",
    "\n",
    "# Hyperparameters to tune\n",
    "\n",
    "learning_rates = [2e-5, 3e-5, 4e-5]\n",
    "full_batch_sizes = [8, 16, 32]\n",
    "lora_batch_sizes = [8, 16]\n",
    "epochs_list = [3, 4]\n",
    "lora_dropouts = [0.05, 0.1]\n",
    "\n",
    "# For fine-tuning, vary fine-tuning one factor at a time first\n",
    "# For full fine-tuning\n",
    "print(\"Full fine-tuning\")\n",
    "\n",
    "# 1. Learning rate\n",
    "for lr in learning_rates:\n",
    "    print(f\"FULL: lr={lr}, bs={baseline_full_bs}, epochs={baseline_epochs}\")\n",
    "    result = run_experiment(\n",
    "        model_name=\"roberta-base\",\n",
    "        strategy=\"full\",\n",
    "        learning_rate=lr,\n",
    "        batch_size=baseline_full_bs,\n",
    "        epochs=baseline_epochs,\n",
    "        lora_config=None\n",
    "    )\n",
    "    roberta_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_roberta_results\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\",\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\",\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenizer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"roberta-base\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"full\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1e-05,\n        \"min\": 2e-05,\n        \"max\": 4e-05,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 16,\n        \"max\": 16,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.094330808029415,\n        \"min\": 345.09836864471436,\n        \"max\": 389.2043459415436,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          379.02744340896606\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0038265306122448606,\n        \"min\": 0.7053571428571429,\n        \"max\": 0.7130102040816326,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7091836734693877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004572886496407564,\n        \"min\": 0.6092896174863388,\n        \"max\": 0.6174863387978142,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6169014084507042\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011292233389981502,\n        \"min\": 0.7041800643086816,\n        \"max\": 0.7266881028938906,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7041800643086816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005472668031647835,\n        \"min\": 0.6576576576576577,\n        \"max\": 0.6676514032496307,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6576576576576577\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003962360900996011,\n        \"min\": 0.6997647573718475,\n        \"max\": 0.7075630753621891,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7024430195161903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004947261826970499,\n        \"min\": 0.7748244427374016,\n        \"max\": 0.78453872456714,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.78453872456714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confusion_matrix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_roberta_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-84ac8a37-fe5e-4b2b-820c-48406e0534df\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>strategy</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>379.027443</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.616901</td>\n",
       "      <td>0.704180</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>0.784539</td>\n",
       "      <td>[[337, 136], [92, 219]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>345.098369</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.609290</td>\n",
       "      <td>0.717042</td>\n",
       "      <td>0.658789</td>\n",
       "      <td>0.699765</td>\n",
       "      <td>0.774824</td>\n",
       "      <td>[[330, 143], [88, 223]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>389.204346</td>\n",
       "      <td>0.713010</td>\n",
       "      <td>0.617486</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>0.667651</td>\n",
       "      <td>0.707563</td>\n",
       "      <td>0.781310</td>\n",
       "      <td>[[333, 140], [85, 226]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84ac8a37-fe5e-4b2b-820c-48406e0534df')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-84ac8a37-fe5e-4b2b-820c-48406e0534df button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-84ac8a37-fe5e-4b2b-820c-48406e0534df');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-edb1772b-16ea-4401-9041-f4adc262ae30\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-edb1772b-16ea-4401-9041-f4adc262ae30')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-edb1772b-16ea-4401-9041-f4adc262ae30 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_23e9e0fa-ce8e-42e5-8bcd-b99aa221da63\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_roberta_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_23e9e0fa-ce8e-42e5-8bcd-b99aa221da63 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_roberta_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                mode  \\\n",
       "0  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "1  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "2  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "\n",
       "                                           tokenizer         model strategy  \\\n",
       "0  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "1  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "2  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "\n",
       "   learning_rate  batch_size  epochs  train_time_s  accuracy  precision  \\\n",
       "0        0.00002          16       3    379.027443  0.709184   0.616901   \n",
       "1        0.00003          16       3    345.098369  0.705357   0.609290   \n",
       "2        0.00004          16       3    389.204346  0.713010   0.617486   \n",
       "\n",
       "     recall        f1  f1_macro   roc_auc         confusion_matrix  \n",
       "0  0.704180  0.657658  0.702443  0.784539  [[337, 136], [92, 219]]  \n",
       "1  0.717042  0.658789  0.699765  0.774824  [[330, 143], [88, 223]]  \n",
       "2  0.726688  0.667651  0.707563  0.781310  [[333, 140], [85, 226]]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roberta_results = pd.DataFrame(roberta_results)\n",
    "df_roberta_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL: lr=4e-05, bs=8, epochs=3\n",
      "\n",
      "Training roberta-base | full | lr=4e-05 | batch_size=8 | epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1074' max='1074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1074/1074 05:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.651900</td>\n",
       "      <td>0.615022</td>\n",
       "      <td>0.673298</td>\n",
       "      <td>0.615756</td>\n",
       "      <td>0.839912</td>\n",
       "      <td>0.710575</td>\n",
       "      <td>0.667788</td>\n",
       "      <td>0.795169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.562251</td>\n",
       "      <td>0.747644</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.738328</td>\n",
       "      <td>0.747324</td>\n",
       "      <td>0.829721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.801360</td>\n",
       "      <td>0.745550</td>\n",
       "      <td>0.702087</td>\n",
       "      <td>0.811404</td>\n",
       "      <td>0.752798</td>\n",
       "      <td>0.745331</td>\n",
       "      <td>0.846526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL: lr=4e-05, bs=32, epochs=3\n",
      "\n",
      "Training roberta-base | full | lr=4e-05 | batch_size=32 | epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 06:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596245</td>\n",
       "      <td>0.698429</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.638158</td>\n",
       "      <td>0.668966</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.765755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.554786</td>\n",
       "      <td>0.722513</td>\n",
       "      <td>0.652313</td>\n",
       "      <td>0.896930</td>\n",
       "      <td>0.755309</td>\n",
       "      <td>0.717437</td>\n",
       "      <td>0.842580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.487200</td>\n",
       "      <td>0.543724</td>\n",
       "      <td>0.754974</td>\n",
       "      <td>0.715116</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.754896</td>\n",
       "      <td>0.852248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Batch size (use best LR from previous step manually)\n",
    "best_lr_full = 4e-5\n",
    "full_batch_sizes = [8, 32] # Already have for batch size 16\n",
    "\n",
    "for bs in full_batch_sizes:\n",
    "    print(f\"FULL: lr={best_lr_full}, bs={bs}, epochs={baseline_epochs}\")\n",
    "    result = run_experiment(\n",
    "        model_name=\"roberta-base\",\n",
    "        strategy=\"full\",\n",
    "        learning_rate=best_lr_full,\n",
    "        batch_size=bs,\n",
    "        epochs=baseline_epochs,\n",
    "        lora_config=None\n",
    "    )\n",
    "    roberta_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_roberta_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\",\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\",\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenizer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"roberta-base\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"full\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.94427190999916e-06,\n        \"min\": 2e-05,\n        \"max\": 4e-05,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 8,\n        \"max\": 32,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19.33948683098949,\n        \"min\": 345.09836864471436,\n        \"max\": 389.2043459415436,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          345.09836864471436\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.014945801361665343,\n        \"min\": 0.6836734693877551,\n        \"max\": 0.7244897959183674,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7053571428571429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008582355140020667,\n        \"min\": 0.6092896174863388,\n        \"max\": 0.6323119777158774,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6092896174863388\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09364589507642257,\n        \"min\": 0.5112540192926045,\n        \"max\": 0.729903536977492,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7170418006430869\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04701743812226913,\n        \"min\": 0.5618374558303887,\n        \"max\": 0.6776119402985075,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6587887740029542\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02344913985987811,\n        \"min\": 0.6571662329052144,\n        \"max\": 0.7185387095701892,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6997647573718475\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03148747847746483,\n        \"min\": 0.7200600939477781,\n        \"max\": 0.8039468943529364,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7748244427374016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confusion_matrix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_roberta_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-58ed8f67-b7a6-4d8d-8d68-815e713680fe\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>strategy</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>379.027443</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.616901</td>\n",
       "      <td>0.704180</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>0.784539</td>\n",
       "      <td>[[337, 136], [92, 219]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>345.098369</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.609290</td>\n",
       "      <td>0.717042</td>\n",
       "      <td>0.658789</td>\n",
       "      <td>0.699765</td>\n",
       "      <td>0.774824</td>\n",
       "      <td>[[330, 143], [88, 223]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>389.204346</td>\n",
       "      <td>0.713010</td>\n",
       "      <td>0.617486</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>0.667651</td>\n",
       "      <td>0.707563</td>\n",
       "      <td>0.781310</td>\n",
       "      <td>[[333, 140], [85, 226]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>347.295208</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.511254</td>\n",
       "      <td>0.561837</td>\n",
       "      <td>0.657166</td>\n",
       "      <td>0.720060</td>\n",
       "      <td>[[377, 96], [152, 159]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>362.918413</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.632312</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.677612</td>\n",
       "      <td>0.718539</td>\n",
       "      <td>0.803947</td>\n",
       "      <td>[[341, 132], [84, 227]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58ed8f67-b7a6-4d8d-8d68-815e713680fe')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-58ed8f67-b7a6-4d8d-8d68-815e713680fe button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-58ed8f67-b7a6-4d8d-8d68-815e713680fe');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-dcb03a05-e30a-4774-b0b4-707400682d22\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcb03a05-e30a-4774-b0b4-707400682d22')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-dcb03a05-e30a-4774-b0b4-707400682d22 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_d4b5a220-59e6-4732-ad2d-5f0f9641fbb2\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_roberta_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_d4b5a220-59e6-4732-ad2d-5f0f9641fbb2 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_roberta_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                mode  \\\n",
       "0  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "1  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "2  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "3  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "4  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "\n",
       "                                           tokenizer         model strategy  \\\n",
       "0  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "1  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "2  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "3  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "4  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "\n",
       "   learning_rate  batch_size  epochs  train_time_s  accuracy  precision  \\\n",
       "0        0.00002          16       3    379.027443  0.709184   0.616901   \n",
       "1        0.00003          16       3    345.098369  0.705357   0.609290   \n",
       "2        0.00004          16       3    389.204346  0.713010   0.617486   \n",
       "3        0.00004           8       3    347.295208  0.683673   0.623529   \n",
       "4        0.00004          32       3    362.918413  0.724490   0.632312   \n",
       "\n",
       "     recall        f1  f1_macro   roc_auc         confusion_matrix  \n",
       "0  0.704180  0.657658  0.702443  0.784539  [[337, 136], [92, 219]]  \n",
       "1  0.717042  0.658789  0.699765  0.774824  [[330, 143], [88, 223]]  \n",
       "2  0.726688  0.667651  0.707563  0.781310  [[333, 140], [85, 226]]  \n",
       "3  0.511254  0.561837  0.657166  0.720060  [[377, 96], [152, 159]]  \n",
       "4  0.729904  0.677612  0.718539  0.803947  [[341, 132], [84, 227]]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roberta_results = pd.DataFrame(roberta_results)\n",
    "df_roberta_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training roberta-base | full | lr=4e-05 | batch_size=32 | epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 09:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596245</td>\n",
       "      <td>0.698429</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.638158</td>\n",
       "      <td>0.668966</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.765755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.600163</td>\n",
       "      <td>0.680628</td>\n",
       "      <td>0.609579</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.733624</td>\n",
       "      <td>0.667466</td>\n",
       "      <td>0.833074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.547239</td>\n",
       "      <td>0.773822</td>\n",
       "      <td>0.757511</td>\n",
       "      <td>0.774123</td>\n",
       "      <td>0.765727</td>\n",
       "      <td>0.773552</td>\n",
       "      <td>0.846781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.764398</td>\n",
       "      <td>0.724272</td>\n",
       "      <td>0.817982</td>\n",
       "      <td>0.768280</td>\n",
       "      <td>0.764332</td>\n",
       "      <td>0.853307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Epochs (use best LR & batch size)\n",
    "best_bs_full = 32\n",
    "result = run_experiment(\n",
    "        model_name=\"roberta-base\",\n",
    "        strategy=\"full\",\n",
    "        learning_rate=best_lr_full,\n",
    "        batch_size=best_bs_full,\n",
    "        # Just need for epoch 4\n",
    "        epochs=4,\n",
    "        lora_config=None)\n",
    "\n",
    "roberta_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_roberta_results\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\",\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\",\n          \"RobertaForSequenceClassification(\\n  (roberta): RobertaModel(\\n    (embeddings): RobertaEmbeddings(\\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\\n      (token_type_embeddings): Embedding(1, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): RobertaEncoder(\\n      (layer): ModuleList(\\n        (0-11): 12 x RobertaLayer(\\n          (attention): RobertaAttention(\\n            (self): RobertaSdpaSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): RobertaSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): RobertaIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n            (intermediate_act_fn): GELUActivation()\\n          )\\n          (output): RobertaOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n  )\\n  (classifier): RobertaClassificationHead(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\\n  )\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenizer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\",\n          \"RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\\n\\t0: AddedToken(\\\"<s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t1: AddedToken(\\\"<pad>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t2: AddedToken(\\\"</s>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t3: AddedToken(\\\"<unk>\\\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\\n\\t50264: AddedToken(\\\"<mask>\\\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\\n}\\n)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"roberta-base\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"full\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.366600265340757e-06,\n        \"min\": 2e-05,\n        \"max\": 4e-05,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 8,\n        \"max\": 32,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85.66458596579693,\n        \"min\": 345.09836864471436,\n        \"max\": 570.2209260463715,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          379.02744340896606\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013369553746281331,\n        \"min\": 0.6836734693877551,\n        \"max\": 0.7244897959183674,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7091836734693877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009730614439063718,\n        \"min\": 0.6092896174863388,\n        \"max\": 0.6345514950166113,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.6169014084507042\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08769945415895201,\n        \"min\": 0.5112540192926045,\n        \"max\": 0.729903536977492,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7041800643086816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.042880462268174616,\n        \"min\": 0.5618374558303887,\n        \"max\": 0.6776119402985075,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.6576576576576577\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021084716951100507,\n        \"min\": 0.6571662329052144,\n        \"max\": 0.7185387095701892,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7024430195161903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028192909556030283,\n        \"min\": 0.7200600939477781,\n        \"max\": 0.8039468943529364,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.78453872456714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confusion_matrix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_roberta_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-2e91bc8d-59f9-4488-b445-a71fb946d598\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>strategy</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>379.027443</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.616901</td>\n",
       "      <td>0.704180</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>0.784539</td>\n",
       "      <td>[[337, 136], [92, 219]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>345.098369</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.609290</td>\n",
       "      <td>0.717042</td>\n",
       "      <td>0.658789</td>\n",
       "      <td>0.699765</td>\n",
       "      <td>0.774824</td>\n",
       "      <td>[[330, 143], [88, 223]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>389.204346</td>\n",
       "      <td>0.713010</td>\n",
       "      <td>0.617486</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>0.667651</td>\n",
       "      <td>0.707563</td>\n",
       "      <td>0.781310</td>\n",
       "      <td>[[333, 140], [85, 226]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>347.295208</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.511254</td>\n",
       "      <td>0.561837</td>\n",
       "      <td>0.657166</td>\n",
       "      <td>0.720060</td>\n",
       "      <td>[[377, 96], [152, 159]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>362.918413</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.632312</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.677612</td>\n",
       "      <td>0.718539</td>\n",
       "      <td>0.803947</td>\n",
       "      <td>[[341, 132], [84, 227]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RobertaForSequenceClassification(\\n  (roberta)...</td>\n",
       "      <td>RobertaTokenizerFast(name_or_path='roberta-bas...</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>570.220926</td>\n",
       "      <td>0.706633</td>\n",
       "      <td>0.634551</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.624183</td>\n",
       "      <td>0.691799</td>\n",
       "      <td>0.776102</td>\n",
       "      <td>[[363, 110], [120, 191]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e91bc8d-59f9-4488-b445-a71fb946d598')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2e91bc8d-59f9-4488-b445-a71fb946d598 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2e91bc8d-59f9-4488-b445-a71fb946d598');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-77522d90-93d6-4ddc-b74e-823f66c809bf\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77522d90-93d6-4ddc-b74e-823f66c809bf')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-77522d90-93d6-4ddc-b74e-823f66c809bf button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_cec5dcd4-331d-409f-8a2e-18acf5d8c1fc\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_roberta_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_cec5dcd4-331d-409f-8a2e-18acf5d8c1fc button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_roberta_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                mode  \\\n",
       "0  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "1  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "2  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "3  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "4  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "5  RobertaForSequenceClassification(\\n  (roberta)...   \n",
       "\n",
       "                                           tokenizer         model strategy  \\\n",
       "0  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "1  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "2  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "3  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "4  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "5  RobertaTokenizerFast(name_or_path='roberta-bas...  roberta-base     full   \n",
       "\n",
       "   learning_rate  batch_size  epochs  train_time_s  accuracy  precision  \\\n",
       "0        0.00002          16       3    379.027443  0.709184   0.616901   \n",
       "1        0.00003          16       3    345.098369  0.705357   0.609290   \n",
       "2        0.00004          16       3    389.204346  0.713010   0.617486   \n",
       "3        0.00004           8       3    347.295208  0.683673   0.623529   \n",
       "4        0.00004          32       3    362.918413  0.724490   0.632312   \n",
       "5        0.00004          32       4    570.220926  0.706633   0.634551   \n",
       "\n",
       "     recall        f1  f1_macro   roc_auc          confusion_matrix  \n",
       "0  0.704180  0.657658  0.702443  0.784539   [[337, 136], [92, 219]]  \n",
       "1  0.717042  0.658789  0.699765  0.774824   [[330, 143], [88, 223]]  \n",
       "2  0.726688  0.667651  0.707563  0.781310   [[333, 140], [85, 226]]  \n",
       "3  0.511254  0.561837  0.657166  0.720060   [[377, 96], [152, 159]]  \n",
       "4  0.729904  0.677612  0.718539  0.803947   [[341, 132], [84, 227]]  \n",
       "5  0.614148  0.624183  0.691799  0.776102  [[363, 110], [120, 191]]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roberta_results = pd.DataFrame(roberta_results)\n",
    "df_roberta_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_roberta_summary\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.366600265340757e-06,\n        \"min\": 2e-05,\n        \"max\": 4e-05,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4e-05,\n          2e-05,\n          3e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 8,\n        \"max\": 32,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          32,\n          16,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013369553746281333,\n        \"min\": 0.6836734693877551,\n        \"max\": 0.7244897959183674,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7244897959183674,\n          0.7130102040816326\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009730614439063718,\n        \"min\": 0.6092896174863388,\n        \"max\": 0.6345514950166113,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.6323119777158774,\n          0.6174863387978142\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08769945415895201,\n        \"min\": 0.5112540192926045,\n        \"max\": 0.729903536977492,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.729903536977492,\n          0.7266881028938906\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021084716951100507,\n        \"min\": 0.6571662329052144,\n        \"max\": 0.7185387095701892,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7185387095701892,\n          0.7075630753621891\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85.66458596579693,\n        \"min\": 345.09836864471436,\n        \"max\": 570.2209260463715,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          362.9184126853943,\n          389.2043459415436\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confusion_matrix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_roberta_summary"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-4adca192-0114-47c2-820a-a4c2dd3b9b9b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.632312</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.718539</td>\n",
       "      <td>362.918413</td>\n",
       "      <td>[[341, 132], [84, 227]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.713010</td>\n",
       "      <td>0.617486</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>0.707563</td>\n",
       "      <td>389.204346</td>\n",
       "      <td>[[333, 140], [85, 226]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.616901</td>\n",
       "      <td>0.704180</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>379.027443</td>\n",
       "      <td>[[337, 136], [92, 219]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.609290</td>\n",
       "      <td>0.717042</td>\n",
       "      <td>0.699765</td>\n",
       "      <td>345.098369</td>\n",
       "      <td>[[330, 143], [88, 223]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.706633</td>\n",
       "      <td>0.634551</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.691799</td>\n",
       "      <td>570.220926</td>\n",
       "      <td>[[363, 110], [120, 191]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.511254</td>\n",
       "      <td>0.657166</td>\n",
       "      <td>347.295208</td>\n",
       "      <td>[[377, 96], [152, 159]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4adca192-0114-47c2-820a-a4c2dd3b9b9b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-4adca192-0114-47c2-820a-a4c2dd3b9b9b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-4adca192-0114-47c2-820a-a4c2dd3b9b9b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-092fa180-bd90-4021-93ad-b8518842c078\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-092fa180-bd90-4021-93ad-b8518842c078')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-092fa180-bd90-4021-93ad-b8518842c078 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_7c65b6a3-a04d-4dfb-9d0a-97ac12a04da0\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_roberta_summary')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_7c65b6a3-a04d-4dfb-9d0a-97ac12a04da0 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_roberta_summary');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   learning_rate  batch_size  epochs  accuracy  precision    recall  f1_macro  \\\n",
       "0        0.00004          32       3  0.724490   0.632312  0.729904  0.718539   \n",
       "1        0.00004          16       3  0.713010   0.617486  0.726688  0.707563   \n",
       "2        0.00002          16       3  0.709184   0.616901  0.704180  0.702443   \n",
       "3        0.00003          16       3  0.705357   0.609290  0.717042  0.699765   \n",
       "4        0.00004          32       4  0.706633   0.634551  0.614148  0.691799   \n",
       "5        0.00004           8       3  0.683673   0.623529  0.511254  0.657166   \n",
       "\n",
       "   train_time_s          confusion_matrix  \n",
       "0    362.918413   [[341, 132], [84, 227]]  \n",
       "1    389.204346   [[333, 140], [85, 226]]  \n",
       "2    379.027443   [[337, 136], [92, 219]]  \n",
       "3    345.098369   [[330, 143], [88, 223]]  \n",
       "4    570.220926  [[363, 110], [120, 191]]  \n",
       "5    347.295208   [[377, 96], [152, 159]]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roberta_summary = df_roberta_results[[\"learning_rate\", \"batch_size\", \"epochs\", \"accuracy\", \"precision\", \"recall\", \"f1_macro\",\n",
    "                                         \"train_time_s\", \"confusion_matrix\"]].sort_values(by=\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_roberta_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training roberta-base | full | lr=0.0004 | batch_size=32 | epochs=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5758eb94b3f748c8abd8517c0e5e9e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2862 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648eb622513c45a691d9d1bb9aa03366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8def55246f94bbb980d4cb555866afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 124,647,170/124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 08:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730992</td>\n",
       "      <td>0.573822</td>\n",
       "      <td>0.545624</td>\n",
       "      <td>0.642544</td>\n",
       "      <td>0.590131</td>\n",
       "      <td>0.573146</td>\n",
       "      <td>0.624064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.692454</td>\n",
       "      <td>0.522513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343191</td>\n",
       "      <td>0.483695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.719500</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.522513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343191</td>\n",
       "      <td>0.454899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the best model again\n",
    "\n",
    "best_model = run_experiment(\n",
    "        model_name=\"roberta-base\",\n",
    "        strategy=\"full\",\n",
    "        learning_rate=4e-4,\n",
    "        batch_size=32,\n",
    "        epochs=3,\n",
    "        lora_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b83e6 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b83e6_row0_col0, #T_b83e6_row0_col1, #T_b83e6_row0_col2, #T_b83e6_row1_col0, #T_b83e6_row1_col1, #T_b83e6_row1_col2, #T_b83e6_row2_col0, #T_b83e6_row2_col1, #T_b83e6_row2_col2, #T_b83e6_row3_col0, #T_b83e6_row3_col1, #T_b83e6_row3_col2, #T_b83e6_row4_col0, #T_b83e6_row4_col1, #T_b83e6_row4_col2, #T_b83e6_row5_col0, #T_b83e6_row5_col1, #T_b83e6_row5_col2, #T_b83e6_row6_col0, #T_b83e6_row6_col1, #T_b83e6_row6_col2, #T_b83e6_row7_col0, #T_b83e6_row7_col1, #T_b83e6_row7_col2, #T_b83e6_row8_col0, #T_b83e6_row8_col1, #T_b83e6_row8_col2, #T_b83e6_row9_col0, #T_b83e6_row9_col1, #T_b83e6_row9_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b83e6\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b83e6_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_b83e6_level0_col1\" class=\"col_heading level0 col1\" >true_label</th>\n",
       "      <th id=\"T_b83e6_level0_col2\" class=\"col_heading level0 col2\" >pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_b83e6_row0_col0\" class=\"data row0 col0\" >Just walked in to #Starbucks and asked for a \"tall blonde\" Hahahaha #irony</td>\n",
       "      <td id=\"T_b83e6_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_b83e6_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
       "      <td id=\"T_b83e6_row1_col0\" class=\"data row1 col0\" >So much #sarcasm at work mate 10/10 #boring 100% #dead mate full on #shit absolutely #sleeping mate can't handle the #sarcasm</td>\n",
       "      <td id=\"T_b83e6_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_b83e6_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "      <td id=\"T_b83e6_row2_col0\" class=\"data row2 col0\" >Corny jokes are my absolute favorite</td>\n",
       "      <td id=\"T_b83e6_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_b83e6_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row3\" class=\"row_heading level0 row3\" >8</th>\n",
       "      <td id=\"T_b83e6_row3_col0\" class=\"data row3 col0\" >if Christian expects Fifa to sleep in my bed with me tonight, he's wrong 👿</td>\n",
       "      <td id=\"T_b83e6_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_b83e6_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row4\" class=\"row_heading level0 row4\" >10</th>\n",
       "      <td id=\"T_b83e6_row4_col0\" class=\"data row4 col0\" >Most important thing I've learned in school</td>\n",
       "      <td id=\"T_b83e6_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_b83e6_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row5\" class=\"row_heading level0 row5\" >12</th>\n",
       "      <td id=\"T_b83e6_row5_col0\" class=\"data row5 col0\" >I love context and large ensemble Fridays!!!!! Der my most favourite #Sarcasm #GetTheFuckOut</td>\n",
       "      <td id=\"T_b83e6_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "      <td id=\"T_b83e6_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row6\" class=\"row_heading level0 row6\" >15</th>\n",
       "      <td id=\"T_b83e6_row6_col0\" class=\"data row6 col0\" >Always classy, never trashy and just a little sassy.</td>\n",
       "      <td id=\"T_b83e6_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_b83e6_row6_col2\" class=\"data row6 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row7\" class=\"row_heading level0 row7\" >16</th>\n",
       "      <td id=\"T_b83e6_row7_col0\" class=\"data row7 col0\" >you believe you can say something, provide no proof and its a fact, WRONG @user @user</td>\n",
       "      <td id=\"T_b83e6_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_b83e6_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row8\" class=\"row_heading level0 row8\" >18</th>\n",
       "      <td id=\"T_b83e6_row8_col0\" class=\"data row8 col0\" >@user Re: Jamie Grace has Tourette's? Thanks for sharing. I'm about to research her & post my tribute to this Young lady Gospel singer</td>\n",
       "      <td id=\"T_b83e6_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_b83e6_row8_col2\" class=\"data row8 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b83e6_level0_row9\" class=\"row_heading level0 row9\" >19</th>\n",
       "      <td id=\"T_b83e6_row9_col0\" class=\"data row9 col0\" >@user Guess they didn't get the memo reg non-nuclear Baltic sea #sarcasm</td>\n",
       "      <td id=\"T_b83e6_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "      <td id=\"T_b83e6_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7b501c88bfb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the error cases from the best model run\n",
    "\n",
    "best_model_results = []\n",
    "best_model_results.append(best_model)\n",
    "best_model_df = pd.DataFrame(best_model_results)\n",
    "error_cases = best_model_df.loc[0, 'error_cases']\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "error_cases.style.set_properties(**{'text-align': 'left'}).set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
